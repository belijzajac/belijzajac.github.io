---
title: "Outperforming Rayon with OpenMP"
date: 2021-11-16T12:43:20-05:00
publishdate: 2021-11-16
lastmod: 2022-01-16
draft: false
tags: ["rust", "c", "perf", "openmp", "blockchain", "kzg-proofs", "bls12-381"]
---

## Whatâ€™s all the fuss about?

For my Blockchain Technologies course, we had two teams competing against each other to produce the fastest Rust library for kzg commitments. Both of us were using the same backend, that is, [blst](https://github.com/supranational/blst) (implemented in assembly but had direct bindings for Rust and C). The first team,  [blst-from-scratch](https://github.com/sifraitech/kzg/tree/main/blst-from-scratch), was using the said Rust bindings to produce an interface closer to [c-kzg](https://github.com/benjaminion/c-kzg), whereas the [ckzg](https://github.com/sifraitech/kzg/tree/main/ckzg) team, which I was part of, was responsible for porting the latter over to Rust.

## Choosing the right tool for the job

It's kind of obvious for Rust programmers to pick `Rayon` out of the box because there aren't any other viable options  for writing parallel code, except for `std::thread` but who wants to manually create and manage threads when there exists simpler solutions, anyway? I had to make a decision which technique to go by:

* Use `pthread`s manually
* Some random guy's threadpool library from GitHub with the most stars
* `OpenMP`

I picked `OpenMP` because while experimenting, I found it to produce the best results, and it was a no-brainer to use except for coming up with how to seemingly integrate it with Rust so that it would work on multiple platforms. In the end, I came up with a simple Bash script to automate the whole building and packaging of shared libraries, and, luckily enough, OpenMP was easily integrated into Rust by either exporting the `RUSTFLAGS` environment variable pointing to the correct `libomp` LLVM runtime or creating a `.cargo/config.toml` file inside the project and mentioning it there. Simple!

## Searching for bottlenecks

The general procedure for looking for what to parallelize is to use CPU profiling tools such as `Perf` that produce flamegraphs, which are a really nice visual way to represent the CPU time of a program. Below is the flamegraph that was generated by running the c-kzg's `fft_g1` benchmark:

![flamegraph-of-fft-g1](/post-images/flame-graphu.svg)

which, as I've noticed, had a huge impact on performance with the majority of calls tracing down to assembly code; a similar kzg implementation library ([go-kzg](https://github.com/protolambda/go-kzg/blob/master/BENCH.md)) that was taken as a reference a few times while producing unit tests and benchmarks, showed that their `fft_g1` benchmark took the longest time to execute as well, among others.

So, we have 7 groups of benchmarks

* das
* fft
* fk20
* kzg
* poly
* recover
* zero_poly

of which let's pick `fft_g1` from the `fft` group to parallelize out!

## Parallelizing fft_g1

The `fft_g1` function in all cases calls the `fft_g1_fast` function, which applies the *divide-and-conquer* principle to divide a large problem into smaller subproblems, recursively solving each of them. The general procedure here is to distribute work (`fft_f1_fast`s) among worker threads.

The blst-from-scratch team implemented it as follows:

```rust {hl_lines=[2,3,4,5]}
let (lo, hi) = ret.split_at_mut(half);
rayon::join(
  || fft_g1_fast(lo, data, stride * 2, roots, roots_stride * 2),
  || fft_g1_fast(hi, &data[stride..], stride * 2, roots, roots_stride * 2)
);

for i in 0..half {
  let y_times_root = ret[i + half].mul(&roots[i * roots_stride]);
  ret[i + half] = ret[i].sub(&y_times_root);
  ret[i] = ret[i].add_or_dbl(&y_times_root);
}
```

As a side note, `rayon::join` spawns two threads, one executing each of the two closures.

The C equivalent, on the other hand, was as follows:

```c {hl_lines=[1,3,7,12,13]}
#pragma omp parallel sections
{
  #pragma omp section
  {
    fft_g1_fast(out, in, stride * 2, roots, roots_stride * 2, half);
  }
  #pragma omp section
  {
    fft_g1_fast(out + half, in + stride, stride * 2, roots, roots_stride * 2, half);
  }
}
#pragma omp parallel
#pragma omp for
for (uint64_t i = 0; i < half; i++) {
  g1_t y_times_root;
  g1_mul(&y_times_root, &out[i + half], &roots[i * roots_stride]);
  g1_sub(&out[i + half], &out[i], &y_times_root);
  g1_add_or_dbl(&out[i], &out[i], &y_times_root);
}
```

In addition to parallel sections, I also utilized OpenMP's parallel for-loop, because I noticed it yielded a **5% greater performance** on my personal machine. Considering the `ubuntu-latest` runner in GitHub Actions CI had only two available threads, the halves of the problem were shared among those two threads where each ran the for-loop to do arithmetic operations on polynomial `G1` points.

In the above code snippets, `fft_g1` calls `fft_g1_fast`, which up to scale 16 should less than or exactly `1 << 15` times recursively call itself, where each such call will be distributed among the 2 threads. Since we're computing `fft_g1` up to scale 8, there should be `(1 << 7) + 1` tasks (not to be confused by OpenMP's `task` pragma directive!) for `fft_g1_fast` or `129` such tasks that will be run in parallel!

## Local c-kzg benchmark

Running on my personal laptop with i5-7300HQ (4 threads overclocked @ 3.50GHz), all mitigations turned off,  and a custom Liquorix kernel, I was able to produce the following results:

<table>
<tr><th>Original c-kzg library</th><th>Parallelized c-kzg library</th></tr>
<tr><td>

```bash {hl_lines=[14]}
$ ./fft_g1_bench
*** Benchmarking FFT_g1, 1 second per test.       
fft_g1/scale_4 1729769 ns/op
fft_g1/scale_5 4935085 ns/op
fft_g1/scale_6 12897731 ns/op
fft_g1/scale_7 32022026 ns/op
fft_g1/scale_8 76552852 ns/op
fft_g1/scale_9 184970057 ns/op
fft_g1/scale_10 418273808 ns/op
fft_g1/scale_11 919499032 ns/op
fft_g1/scale_12 2025633037 ns/op
fft_g1/scale_13 4479830518 ns/op
fft_g1/scale_14 9754557496 ns/op
fft_g1/scale_15 21125613058 ns/op
```

</td><td>

```bash {hl_lines=[14]}
$ OMP_NUM_THREADS=4 ./fft_g1_bench
*** Benchmarking FFT_g1, 1 second per test.       
fft_g1/scale_4 839454 ns/op
fft_g1/scale_5 2378457 ns/op
fft_g1/scale_6 6404191 ns/op
fft_g1/scale_7 16325966 ns/op
fft_g1/scale_8 38141754 ns/op
fft_g1/scale_9 90948810 ns/op
fft_g1/scale_10 204757690 ns/op
fft_g1/scale_11 457509973 ns/op
fft_g1/scale_12 1006089135 ns/op
fft_g1/scale_13 2240095284 ns/op
fft_g1/scale_14 4879448286 ns/op
fft_g1/scale_15 10650876381 ns/op
```

</td></tr> </table>

That's **twice as fast** with as little effort as putting in a few pragmas!

## GitHub Actions benchmarks

The `fft_g1` benchmark was limited to scale 7 because the overall run time for the job exceeds the 6 hour limit if I were to benchmark it  up to scale 16, as Criterion runs each iteration a couple of hundred times to produce more accurate results, and that used to automatically cancel other running tasks as jobs submitted to GitHub Actions have a limit of 360 minutes.

### 1. blst-from-scratch benchmark

![from-scratch-github-actions](/post-images/from-scratch-github-actions.png)

From the above screenshot we can see that the parallelized version of the produced library ran `1m 28s` shorter than its sequential version, and below are the results of sequential `fft_g1` algorithm:

```text {hl_lines=[5]}
Benchmarking bench_fft_g1 scale: '7'
Benchmarking bench_fft_g1 scale: '7': Warming up for 3.0000 s
Benchmarking bench_fft_g1 scale: '7': Collecting 100 samples in estimated 6.6364 s (200 iterations)
Benchmarking bench_fft_g1 scale: '7': Analyzing
bench_fft_g1 scale: '7' time:   [33.423 ms 33.785 ms 34.150 ms]
```

of which the average run time for scale 7 was cut down by `38.926%` by its parallel counterpart:

```text {hl_lines=[5,6]}
Benchmarking bench_fft_g1 scale: '7'
Benchmarking bench_fft_g1 scale: '7': Warming up for 3.0000 s
Benchmarking bench_fft_g1 scale: '7': Collecting 100 samples in estimated 6.3282 s (300 iterations)
Benchmarking bench_fft_g1 scale: '7': Analyzing
bench_fft_g1 scale: '7' time:   [20.432 ms 20.634 ms 20.843 ms]
                        change: [-39.822% -38.926% -38.001%] (p = 0.00 < 0.05)
                        Performance has improved.
```

### 2. ckzg benchmark

![ckzg-github-actions](/post-images/ckzg-github-actions.png)

The sequential version of the ckzg library ran `2m 7s` faster than the same version of blst-from-scratch because it had other benchmarks that performed faster, though the parallelized version ran `1m 2s` faster than its sequential version. Below are the results of the sequantial `fft_g1` algorithm:

```text {hl_lines=[5]}
Benchmarking bench_fft_g1 scale: '7'
Benchmarking bench_fft_g1 scale: '7': Warming up for 3.0000 s
Benchmarking bench_fft_g1 scale: '7': Collecting 100 samples in estimated 6.8313 s (200 iterations)
Benchmarking bench_fft_g1 scale: '7': Analyzing
bench_fft_g1 scale: '7' time:   [32.194 ms 32.471 ms 32.760 ms]
```

Yet the parallel version of the `fft_g1` algorithm performed much faster than it did for blst-from-scratch even though both unparallelized versions performed evenly:

```text {hl_lines=[5,6]}
Benchmarking bench_fft_g1 scale: '7'
Benchmarking bench_fft_g1 scale: '7': Warming up for 3.0000 s
Benchmarking bench_fft_g1 scale: '7': Collecting 100 samples in estimated 5.0701 s (300 iterations)
Benchmarking bench_fft_g1 scale: '7': Analyzing
bench_fft_g1 scale: '7' time:   [16.854 ms 17.107 ms 17.439 ms]
                        change: [-48.216% -47.318% -46.306%] (p = 0.00 < 0.05)
                        Performance has improved.
```

To cut a long story short, ckzg outperformed blst-from-scratch in all of the 7 benchmark groups.

## Summary

* OpenMP lets quickly propotype what is feasible to parallelize out with the help of CPU profiling tools such as Perf
* Criterion is actually a really nice benchmarking tool for measuring performance, especially when integrated into CI
* ckzg has surpassed blst-from-scratch in becoming the fastest Rust library (yet) for kzg10 commitments
