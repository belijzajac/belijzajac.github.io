---
title: "Outperforming Rayon with OpenMP"
description: "Replacing Rayon with OpenMP for additional gains"
date: 2021-11-16
tags: ["rust", "c", "perf", "openmp", "kzg-proofs", "bls12-381"]
---

![rip-craberino](/post-images/rip-craberino.jpg)

## Introduction

For the Blockchain Technologies course, students were paired into groups and assigned to produce the fastest Rust library implementing the KZG10  cryptographic scheme. Two teams used the <highlight>[blst](https://github.com/supranational/blst)</highlight> backend, which is implemented in assembly and has direct bindings for Rust and C. The first team, <highlight>[blst-from-scratch](https://github.com/grandinetech/rust-kzg/tree/main/blst)</highlight>, used the Rust bindings provided by the blst library to produce an interface closer to <highlight>[c-kzg](https://github.com/benjaminion/c-kzg)</highlight>. The second team, which I was part of, worked on the <highlight>[ckzg](https://github.com/grandinetech/rust-kzg/tree/main/ckzg)</highlight> library in C. We were responsible for producing an implementation that could integrate into Rust via the C bindings provided by my team.

## Choosing the right tool for the job

It's a no-brainer for Rust programmers to choose `Rayon` when it comes to writing parallel code, as there aren't many other viable and easy-to-use options available. While Rust does offer alternatives like `std::thread`, which provides access to native OS threads, the manual creation and management of threads can be cumbersome.

When I was working on my C code, I had to decide on the best approach to parallelize it. My options included:

* `pthread`: A POSIX standard for thread creation and management.
* A popular third-party threadpool library from GitHub.
* `OpenMP`: Parallel programming library for C and C++ without manual thread management.

I chose OpenMP because, during experimentation, I discovered it yielded the best results and was relatively straightforward to use. However, I encountered a challenge in integrating it with Rust to ensure compatibility across multiple platforms, starting with Linux and possibly macOS. Eventually, I came up with the following Bash script to automate the entire process of building and packaging shared libraries. Fortunately, OpenMP was integrated into Rust by either:

* exporting the `RUSTFLAGS` environment variable pointing to the correct `libomp` LLVM runtime

```bash
# Linux
apt install libomp-dev
export LIBOMP_PATH=$(find /usr/lib/llvm* -name libiomp5.so | head -n 1)

# MacOS
brew install libomp
ln -s /usr/local/opt/libomp/lib/libomp.dylib /usr/local/lib
ln -s /usr/local/opt/libomp/include/omp.h /usr/local/include
export LIBOMP_PATH=/usr/local/lib/libomp.dylib

# And finally
export RUSTFLAGS="-C link-arg=$LIBOMP_PATH"
```

* or creating a `.cargo/config.toml` file inside the project directory and mentioning it there

```text
[build]
rustflags = [
  "-C", "link-arg=LIBOMP_PATH"
]
```

Well, that was simple.

## Searching for bottlenecks

In order to optimize a program's performance, CPU profiling tools like `Perf` play a crucial role by providing detailed insights into where computational resources are being used. One powerful visualization tool generated by these profilers is the flamegraph, which offers a clear representation of a program's CPU usage over time.

![flamegraph-of-fft-g1](/post-images/flame-graphu.svg)

The flamegraph displayed above illustrates the CPU time distribution of the c-kzg library's `fft_g1` benchmark. Upon analysis, it became evident that a significant portion of the execution time was spent in assembly code, highlighting potential areas for optimization. Further investigation on <highlight>[go-kzg](https://github.com/protolambda/go-kzg)</highlight> revealed that the `fft_g1` benchmark was indeed a performance bottleneck and stood out as a prime candidate for parallelization. By parallelizing this specific operation, we can improving the overall performance of the library.

## Parallelizing fft_g1

The `fft_g1` function calls the `fft_g1_fast` function, which applies the *divide-and-conquer* principle to divide a large problem into smaller subproblems, recursively solving each of them. The general procedure here is to distribute work (`fft_f1_fast`s) among worker threads.

The blst-from-scratch team implemented it as follows:

```rust {hl_lines=[2,3,4,5]}
let (lo, hi) = ret.split_at_mut(half);
rayon::join(
  || fft_g1_fast(lo, data, stride * 2, roots, roots_stride * 2),
  || fft_g1_fast(hi, &data[stride..], stride * 2, roots, roots_stride * 2)
);

for i in 0..half {
  let y_times_root = ret[i + half].mul(&roots[i * roots_stride]);
  ret[i + half] = ret[i].sub(&y_times_root);
  ret[i] = ret[i].add_or_dbl(&y_times_root);
}
```

As a side note, `rayon::join` spawns two threads, one executing each of the two closures.

The C equivalent, on the other hand, was as follows:

```c {hl_lines=[1,3,7,12,13]}
#pragma omp parallel sections
{
  #pragma omp section
  {
    fft_g1_fast(out, in, stride * 2, roots, roots_stride * 2, half);
  }
  #pragma omp section
  {
    fft_g1_fast(out + half, in + stride, stride * 2, roots, roots_stride * 2, half);
  }
}
#pragma omp parallel
#pragma omp for
for (uint64_t i = 0; i < half; i++) {
  g1_t y_times_root;
  g1_mul(&y_times_root, &out[i + half], &roots[i * roots_stride]);
  g1_sub(&out[i + half], &out[i], &y_times_root);
  g1_add_or_dbl(&out[i], &out[i], &y_times_root);
}
```

In addition to parallel sections, I also used OpenMP's parallel for-loop, because I noticed it yielded a **5% greater performance** on my personal machine. Considering the `ubuntu-latest` runner in GitHub Actions CI had only two available cores, the halves of the problem were shared among two threads where each ran the for-loop to do arithmetic operations on polynomial `G1` points.

In the above code snippets, `fft_g1` calls `fft_g1_fast`, which up to scale 16 should at most `1 << 15` times call itself recursively, where each such call will be distributed among the 2 threads. Since we're computing `fft_g1` up to scale 8, there should be `(1 << 7) + 1` tasks (not to be confused by OpenMP's `task` pragma directive!) for `fft_g1_fast` or `129` such tasks that will be run in parallel!

## Local c-kzg benchmark

Running on my personal computer with i5-7300HQ (4 threads overclocked at 3.50GHz), all mitigations turned off, and a custom Liquorix kernel, I was able to achieve the following results:

<table>
<tr><th>Original c-kzg library</th><th>Parallelized c-kzg library</th></tr>
<tr><td>

```bash {hl_lines=[14]}
$ ./fft_g1_bench
*** Benchmarking FFT_g1, 1 second per test.       
fft_g1/scale_4 1729769 ns/op
fft_g1/scale_5 4935085 ns/op
fft_g1/scale_6 12897731 ns/op
fft_g1/scale_7 32022026 ns/op
fft_g1/scale_8 76552852 ns/op
fft_g1/scale_9 184970057 ns/op
fft_g1/scale_10 418273808 ns/op
fft_g1/scale_11 919499032 ns/op
fft_g1/scale_12 2025633037 ns/op
fft_g1/scale_13 4479830518 ns/op
fft_g1/scale_14 9754557496 ns/op
fft_g1/scale_15 21125613058 ns/op
```

</td><td>

```bash {hl_lines=[14]}
$ OMP_NUM_THREADS=4 ./fft_g1_bench
*** Benchmarking FFT_g1, 1 second per test.       
fft_g1/scale_4 839454 ns/op
fft_g1/scale_5 2378457 ns/op
fft_g1/scale_6 6404191 ns/op
fft_g1/scale_7 16325966 ns/op
fft_g1/scale_8 38141754 ns/op
fft_g1/scale_9 90948810 ns/op
fft_g1/scale_10 204757690 ns/op
fft_g1/scale_11 457509973 ns/op
fft_g1/scale_12 1006089135 ns/op
fft_g1/scale_13 2240095284 ns/op
fft_g1/scale_14 4879448286 ns/op
fft_g1/scale_15 10650876381 ns/op
```

</td></tr></table>

That's **twice as fast** with as little effort as putting in a few pragmas!

## GitHub Actions CI benchmarks

The `fft_g1` benchmark was limited to scale 7 because the overall run time for the job exceeds the 6 hour limit if I were to benchmark it up to scale 16, as Criterion runs each iteration a couple of hundred times to produce more accurate results, and that used to automatically cancel other running CI jobs as jobs submitted to GitHub Actions are limited to 360 minutes.

### Benchmarking blst-from-scratch

![from-scratch-github-actions](/post-images/from-scratch-github-actions.png)

From the above screenshot we can see that the parallelized version of the library ran `1m 28s` shorter than its sequential version, and below are the results of sequential `fft_g1` algorithm:

```text {hl_lines=[5]}
Benchmarking bench_fft_g1 scale: '7'
Benchmarking bench_fft_g1 scale: '7': Warming up for 3.0000 s
Benchmarking bench_fft_g1 scale: '7': Collecting 100 samples in estimated 6.6364 s (200 iterations)
Benchmarking bench_fft_g1 scale: '7': Analyzing
bench_fft_g1 scale: '7' time:   [33.423 ms 33.785 ms 34.150 ms]
```

of which the average run time for scale 7 was cut down by `38.926%` by its parallel counterpart:

```text {hl_lines=[5,6]}
Benchmarking bench_fft_g1 scale: '7'
Benchmarking bench_fft_g1 scale: '7': Warming up for 3.0000 s
Benchmarking bench_fft_g1 scale: '7': Collecting 100 samples in estimated 6.3282 s (300 iterations)
Benchmarking bench_fft_g1 scale: '7': Analyzing
bench_fft_g1 scale: '7' time:   [20.432 ms 20.634 ms 20.843 ms]
                        change: [-39.822% -38.926% -38.001%] (p = 0.00 < 0.05)
                        Performance has improved.
```

### Benchmarking ckzg

![ckzg-github-actions](/post-images/ckzg-github-actions.png)

The sequential version of the ckzg library ran `2m 7s` faster than the same version of blst-from-scratch because it had other benchmarks that performed faster, though the parallelized version ran `1m 2s` faster than its sequential version. Below are the results of the sequantial `fft_g1` algorithm:

```text {hl_lines=[5]}
Benchmarking bench_fft_g1 scale: '7'
Benchmarking bench_fft_g1 scale: '7': Warming up for 3.0000 s
Benchmarking bench_fft_g1 scale: '7': Collecting 100 samples in estimated 6.8313 s (200 iterations)
Benchmarking bench_fft_g1 scale: '7': Analyzing
bench_fft_g1 scale: '7' time:   [32.194 ms 32.471 ms 32.760 ms]
```

Yet the parallel version of the `fft_g1` algorithm performed much faster than it did for blst-from-scratch, even though both unparallelized versions for both teams performed evenly:

```text {hl_lines=[5,6]}
Benchmarking bench_fft_g1 scale: '7'
Benchmarking bench_fft_g1 scale: '7': Warming up for 3.0000 s
Benchmarking bench_fft_g1 scale: '7': Collecting 100 samples in estimated 5.0701 s (300 iterations)
Benchmarking bench_fft_g1 scale: '7': Analyzing
bench_fft_g1 scale: '7' time:   [16.854 ms 17.107 ms 17.439 ms]
                        change: [-48.216% -47.318% -46.306%] (p = 0.00 < 0.05)
                        Performance has improved.
```

## Summary

* OpenMP lets you quickly prototype what is possible to parallelize with the help of CPU profiling tools like Perf
* Criterion is actually a really nice benchmarking tool to measure performance, especially when integrated into CI
