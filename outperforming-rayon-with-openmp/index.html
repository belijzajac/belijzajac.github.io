<!doctype html><html lang=en><head><meta charset=utf-8><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=generator content="Hugo 0.105.0"><meta name=description content="belijzajac's personal blog, where he shows off some of his current, past, and future projects as well as shares personal experience of developing projects involving modern C++ and Linux!"><link rel=apple-touch-icon sizes=180x180 href=https://belijzajac.dev/apple-touch-icon.png><link rel=icon type=image/png sizes=32x32 href=https://belijzajac.dev/favicon-32x32.png><link rel=icon type=image/png sizes=16x16 href=https://belijzajac.dev/favicon-16x16.png><link rel=manifest href=https://belijzajac.dev/site.webmanifest><link rel=mask-icon href=https://belijzajac.dev/safari-pinned-tab.svg color=#5bbad5><meta name=msapplication-TileColor content="#da532c"><meta name=theme-color content="#ffffff"><link rel=stylesheet href=https://belijzajac.dev/css/bootstrap.min.css><title>Outperforming Rayon with OpenMP | belijzajac.dev</title><style>body{min-width:300px}.custom-navbar{margin-bottom:1em;height:60px}.custom-navbar a{display:inline-block;padding:18px 0;margin-right:1em;font-weight:700}.custom-navbar a:hover,.custom-navbar a:focus{text-decoration:none}@media print{.custom-navbar{display:none}}article{padding-bottom:1em}img{max-width:100%}body{background-image:url(/white-leather-texture-1945.png);background-repeat:repeat}body{background-color:#fff}body{color:#212529}a{color:#007bff}a:hover,a:focus{color:#0056b3}.custom-navbar{background-color:#212529}.custom-navbar a{color:rgba(255,255,255,.75)}.custom-navbar a:hover,.custom-navbar a:focus{color:#fff}.container{max-width:900px}pre{display:block;padding:9.5px;word-break:break-all;word-wrap:break-word;background-color:#f5f5f5}pre code{padding:0;font-size:inherit;color:inherit;white-space:pre-wrap;background-color:transparent;border:none;border-radius:0}code{padding:2px 4px;color:inherit;background-color:#f5f5f5;border:1px solid #ccc;border-radius:4px;font-size:.9em}blockquote,.blockquote{padding:10px 20px;margin:0 0 20px;font-size:1em;border-left:5px solid #6c757d}.footer{text-align:center;color:#b3b3b3}.footer a{color:inherit}.separator{border:0;border-top:3px dashed #000}.email{background-color:#101010;color:#fff}</style></head><body><nav class=custom-navbar><div class=container><span class=navbar-text><p style=color:#fff;font-size:20px><b>belijzajac.dev</b></p></span><span style=padding-left:20px><a href=/>Posts</a>
<a href=/index.xml>RSS</a></div></nav><div class=container><article><h1>Outperforming Rayon with OpenMP</h1><p><small class=text-secondary>Nov 16, 2021, updated Jan 16, 2022</small>
<small><code><a href=https://belijzajac.dev/tags/rust>rust</a></code></small>
<small><code><a href=https://belijzajac.dev/tags/c>c</a></code></small>
<small><code><a href=https://belijzajac.dev/tags/perf>perf</a></code></small>
<small><code><a href=https://belijzajac.dev/tags/openmp>openmp</a></code></small>
<small><code><a href=https://belijzajac.dev/tags/blockchain>blockchain</a></code></small>
<small><code><a href=https://belijzajac.dev/tags/kzg-proofs>kzg-proofs</a></code></small>
<small><code><a href=https://belijzajac.dev/tags/bls12-381>bls12-381</a></code></small></p><p><img src=/post-images/rip-craberino.jpg alt=rip-craberino></p><h2 id=whats-it-all-about>Whatâ€™s it all about?</h2><p>For the Blockchain Technologies course, students were paired into groups and assigned to produce the fastest Rust library (crate) implementing the KZG10 scheme. Two teams used the same backend, that is, <a href=https://github.com/supranational/blst>blst</a> (implemented in assembly with direct bindings for Rust and C). The first team, <a href=https://github.com/sifraitech/kzg/tree/main/blst-from-scratch>blst-from-scratch</a>, was using the said Rust bindings to produce an interface closer to <a href=https://github.com/benjaminion/c-kzg>c-kzg</a>, whereas the <a href=https://github.com/sifraitech/kzg/tree/main/ckzg>ckzg</a> team, which I was part of, was responsible for porting the latter over to Rust.</p><h2 id=choosing-the-right-tool-for-the-job>Choosing the right tool for the job</h2><p>It&rsquo;s kind of obvious for Rust programmers to pick <code>Rayon</code> out of the box because there aren&rsquo;t any other viable options for writing parallel code, except for <code>std::thread</code>s, but who wants to manually create and manage threads when simpler solutions exist, anyway? I had to make a decision on which technique to go by:</p><ul><li>Use <code>pthread</code>s manually</li><li>Some random guy&rsquo;s threadpool library from GitHub with the most stars</li><li><code>OpenMP</code></li></ul><p>I picked <code>OpenMP</code> because while experimenting, I found it to produce the best results, and it was a no-brainer to use except for coming up with how to seemingly integrate it with Rust so that it would work on multiple platforms. In the end, I came up with a simple Bash script to automate the whole building and packaging of shared libraries, and, luckily enough, OpenMP was easily integrated into Rust by either exporting the <code>RUSTFLAGS</code> environment variable pointing to the correct <code>libomp</code> LLVM runtime or creating a <code>.cargo/config.toml</code> file inside the project and mentioning it there. Simple!</p><h2 id=searching-for-bottlenecks>Searching for bottlenecks</h2><p>The general procedure for looking for what to parallelize is to use CPU profiling tools such as <code>Perf</code> that produce flamegraphs, which are a really nice visual way to represent the CPU time of a program. Below is the flamegraph that was generated by running the c-kzg&rsquo;s <code>fft_g1</code> benchmark:</p><p><img src=/post-images/flame-graphu.svg alt=flamegraph-of-fft-g1></p><p>which, as I&rsquo;ve noticed, had a huge impact on performance with the majority of calls tracing down to assembly code; a similar kzg implementation library (<a href=https://github.com/protolambda/go-kzg/blob/master/BENCH.md>go-kzg</a>) that was taken as a reference a few times while producing unit tests and benchmarks, showed that their <code>fft_g1</code> benchmark took the longest time to execute as well, among others.</p><p>So, we have 7 groups of benchmarks:</p><ul><li>das</li><li>fft</li><li>fk20</li><li>kzg</li><li>poly</li><li>recover</li><li>zero_poly</li></ul><p>of which let&rsquo;s pick <code>fft_g1</code> from the <code>fft</code> group to parallelize out!</p><h2 id=parallelizing-fft_g1>Parallelizing fft_g1</h2><p>The <code>fft_g1</code> function in all cases calls the <code>fft_g1_fast</code> function, which applies the <em>divide-and-conquer</em> principle to divide a large problem into smaller subproblems, recursively solving each of them. The general procedure here is to distribute work (<code>fft_f1_fast</code>s) among worker threads.</p><p>The blst-from-scratch team implemented it as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code class=language-rust data-lang=rust><span style=display:flex><span><span style=color:#8be9fd;font-style:italic>let</span> (lo, hi) <span style=color:#ff79c6>=</span> ret.split_at_mut(half);
</span></span><span style=display:flex;background-color:#3d3f4a><span>rayon::join(
</span></span><span style=display:flex;background-color:#3d3f4a><span>  <span style=color:#ff79c6>||</span> fft_g1_fast(lo, data, stride <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>2</span>, roots, roots_stride <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>2</span>),
</span></span><span style=display:flex;background-color:#3d3f4a><span>  <span style=color:#ff79c6>||</span> fft_g1_fast(hi, <span style=color:#ff79c6>&amp;</span>data[stride<span style=color:#ff79c6>..</span>], stride <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>2</span>, roots, roots_stride <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>2</span>)
</span></span><span style=display:flex;background-color:#3d3f4a><span>);
</span></span><span style=display:flex><span>
</span></span><span style=display:flex><span><span style=color:#ff79c6>for</span> i <span style=color:#ff79c6>in</span> <span style=color:#bd93f9>0</span><span style=color:#ff79c6>..</span>half {
</span></span><span style=display:flex><span>  <span style=color:#8be9fd;font-style:italic>let</span> y_times_root <span style=color:#ff79c6>=</span> ret[i <span style=color:#ff79c6>+</span> half].mul(<span style=color:#ff79c6>&amp;</span>roots[i <span style=color:#ff79c6>*</span> roots_stride]);
</span></span><span style=display:flex><span>  ret[i <span style=color:#ff79c6>+</span> half] <span style=color:#ff79c6>=</span> ret[i].sub(<span style=color:#ff79c6>&amp;</span>y_times_root);
</span></span><span style=display:flex><span>  ret[i] <span style=color:#ff79c6>=</span> ret[i].add_or_dbl(<span style=color:#ff79c6>&amp;</span>y_times_root);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>As a side note, <code>rayon::join</code> spawns two threads, one executing each of the two closures.</p><p>The C equivalent, on the other hand, was as follows:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code class=language-c data-lang=c><span style=display:flex;background-color:#3d3f4a><span><span style=color:#ff79c6>#pragma omp parallel sections
</span></span></span><span style=display:flex><span><span style=color:#ff79c6></span>{
</span></span><span style=display:flex;background-color:#3d3f4a><span>  <span style=color:#ff79c6>#pragma omp section
</span></span></span><span style=display:flex><span><span style=color:#ff79c6></span>  {
</span></span><span style=display:flex><span>    fft_g1_fast(out, in, stride <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>2</span>, roots, roots_stride <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>2</span>, half);
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex;background-color:#3d3f4a><span>  <span style=color:#ff79c6>#pragma omp section
</span></span></span><span style=display:flex><span><span style=color:#ff79c6></span>  {
</span></span><span style=display:flex><span>    fft_g1_fast(out <span style=color:#ff79c6>+</span> half, in <span style=color:#ff79c6>+</span> stride, stride <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>2</span>, roots, roots_stride <span style=color:#ff79c6>*</span> <span style=color:#bd93f9>2</span>, half);
</span></span><span style=display:flex><span>  }
</span></span><span style=display:flex><span>}
</span></span><span style=display:flex;background-color:#3d3f4a><span><span style=color:#ff79c6>#pragma omp parallel
</span></span></span><span style=display:flex;background-color:#3d3f4a><span><span style=color:#ff79c6>#pragma omp for
</span></span></span><span style=display:flex><span><span style=color:#ff79c6></span><span style=color:#ff79c6>for</span> (<span style=color:#8be9fd>uint64_t</span> i <span style=color:#ff79c6>=</span> <span style=color:#bd93f9>0</span>; i <span style=color:#ff79c6>&lt;</span> half; i<span style=color:#ff79c6>++</span>) {
</span></span><span style=display:flex><span>  g1_t y_times_root;
</span></span><span style=display:flex><span>  g1_mul(<span style=color:#ff79c6>&amp;</span>y_times_root, <span style=color:#ff79c6>&amp;</span>out[i <span style=color:#ff79c6>+</span> half], <span style=color:#ff79c6>&amp;</span>roots[i <span style=color:#ff79c6>*</span> roots_stride]);
</span></span><span style=display:flex><span>  g1_sub(<span style=color:#ff79c6>&amp;</span>out[i <span style=color:#ff79c6>+</span> half], <span style=color:#ff79c6>&amp;</span>out[i], <span style=color:#ff79c6>&amp;</span>y_times_root);
</span></span><span style=display:flex><span>  g1_add_or_dbl(<span style=color:#ff79c6>&amp;</span>out[i], <span style=color:#ff79c6>&amp;</span>out[i], <span style=color:#ff79c6>&amp;</span>y_times_root);
</span></span><span style=display:flex><span>}
</span></span></code></pre></div><p>In addition to parallel sections, I also utilized OpenMP&rsquo;s parallel for-loop, because I noticed it yielded a <strong>5% greater performance</strong> on my personal machine. Considering the <code>ubuntu-latest</code> runner in GitHub Actions CI had only two available threads, the halves of the problem were shared among those two threads where each ran the for-loop to do arithmetic operations on polynomial <code>G1</code> points.</p><p>In the above code snippets, <code>fft_g1</code> calls <code>fft_g1_fast</code>, which up to scale 16 should at most <code>1 &lt;&lt; 15</code> times recursively call itself, where each such call will be distributed among the 2 threads. Since we&rsquo;re computing <code>fft_g1</code> up to scale 8, there should be <code>(1 &lt;&lt; 7) + 1</code> tasks (not to be confused by OpenMP&rsquo;s <code>task</code> pragma directive!) for <code>fft_g1_fast</code> or <code>129</code> such tasks that will be run in parallel!</p><h2 id=local-c-kzg-benchmark>Local c-kzg benchmark</h2><p>Running on my personal laptop with i5-7300HQ (4 threads overclocked @ 3.50GHz), all mitigations turned off, and a custom Liquorix kernel, I was able to produce the following results:</p><table><tr><th>Original c-kzg library</th><th>Parallelized c-kzg library</th></tr><tr><td><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code class=language-bash data-lang=bash><span style=display:flex><span>$ ./fft_g1_bench
</span></span><span style=display:flex><span>*** Benchmarking FFT_g1, <span style=color:#bd93f9>1</span> second per test.       
</span></span><span style=display:flex><span>fft_g1/scale_4 <span style=color:#bd93f9>1729769</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_5 <span style=color:#bd93f9>4935085</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_6 <span style=color:#bd93f9>12897731</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_7 <span style=color:#bd93f9>32022026</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_8 <span style=color:#bd93f9>76552852</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_9 <span style=color:#bd93f9>184970057</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_10 <span style=color:#bd93f9>418273808</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_11 <span style=color:#bd93f9>919499032</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_12 <span style=color:#bd93f9>2025633037</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_13 <span style=color:#bd93f9>4479830518</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_14 <span style=color:#bd93f9>9754557496</span> ns/op
</span></span><span style=display:flex;background-color:#3d3f4a><span>fft_g1/scale_15 <span style=color:#bd93f9>21125613058</span> ns/op
</span></span></code></pre></div></td><td><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code class=language-bash data-lang=bash><span style=display:flex><span>$ <span style=color:#8be9fd;font-style:italic>OMP_NUM_THREADS</span><span style=color:#ff79c6>=</span><span style=color:#bd93f9>4</span> ./fft_g1_bench
</span></span><span style=display:flex><span>*** Benchmarking FFT_g1, <span style=color:#bd93f9>1</span> second per test.       
</span></span><span style=display:flex><span>fft_g1/scale_4 <span style=color:#bd93f9>839454</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_5 <span style=color:#bd93f9>2378457</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_6 <span style=color:#bd93f9>6404191</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_7 <span style=color:#bd93f9>16325966</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_8 <span style=color:#bd93f9>38141754</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_9 <span style=color:#bd93f9>90948810</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_10 <span style=color:#bd93f9>204757690</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_11 <span style=color:#bd93f9>457509973</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_12 <span style=color:#bd93f9>1006089135</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_13 <span style=color:#bd93f9>2240095284</span> ns/op
</span></span><span style=display:flex><span>fft_g1/scale_14 <span style=color:#bd93f9>4879448286</span> ns/op
</span></span><span style=display:flex;background-color:#3d3f4a><span>fft_g1/scale_15 <span style=color:#bd93f9>10650876381</span> ns/op
</span></span></code></pre></div></td></tr></table><p>That&rsquo;s <strong>twice as fast</strong> with as little effort as putting in a few pragmas!</p><h2 id=github-actions-ci-benchmarks>GitHub Actions CI benchmarks</h2><p>The <code>fft_g1</code> benchmark was limited to scale 7 because the overall run time for the job exceeds the 6 hour limit if I were to benchmark it up to scale 16, as Criterion runs each iteration a couple of hundred times to produce more accurate results, and that used to automatically cancel other running tasks as jobs submitted to GitHub Actions have a limit of 360 minutes.</p><h3 id=1-blst-from-scratch-benchmark>1. blst-from-scratch benchmark</h3><p><img src=/post-images/from-scratch-github-actions.png alt=from-scratch-github-actions></p><p>From the above screenshot we can see that the parallelized version of the produced library ran <code>1m 28s</code> shorter than its sequential version, and below are the results of sequential <code>fft_g1</code> algorithm:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code class=language-text data-lang=text><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;
</span></span><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;: Warming up for 3.0000 s
</span></span><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;: Collecting 100 samples in estimated 6.6364 s (200 iterations)
</span></span><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;: Analyzing
</span></span><span style=display:flex;background-color:#3d3f4a><span>bench_fft_g1 scale: &#39;7&#39; time:   [33.423 ms 33.785 ms 34.150 ms]
</span></span></code></pre></div><p>of which the average run time for scale 7 was cut down by <code>38.926%</code> by its parallel counterpart:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code class=language-text data-lang=text><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;
</span></span><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;: Warming up for 3.0000 s
</span></span><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;: Collecting 100 samples in estimated 6.3282 s (300 iterations)
</span></span><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;: Analyzing
</span></span><span style=display:flex;background-color:#3d3f4a><span>bench_fft_g1 scale: &#39;7&#39; time:   [20.432 ms 20.634 ms 20.843 ms]
</span></span><span style=display:flex;background-color:#3d3f4a><span>                        change: [-39.822% -38.926% -38.001%] (p = 0.00 &lt; 0.05)
</span></span><span style=display:flex><span>                        Performance has improved.
</span></span></code></pre></div><h3 id=2-ckzg-benchmark>2. ckzg benchmark</h3><p><img src=/post-images/ckzg-github-actions.png alt=ckzg-github-actions></p><p>The sequential version of the ckzg library ran <code>2m 7s</code> faster than the same version of blst-from-scratch because it had other benchmarks that performed faster, though the parallelized version ran <code>1m 2s</code> faster than its sequential version. Below are the results of the sequantial <code>fft_g1</code> algorithm:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code class=language-text data-lang=text><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;
</span></span><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;: Warming up for 3.0000 s
</span></span><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;: Collecting 100 samples in estimated 6.8313 s (200 iterations)
</span></span><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;: Analyzing
</span></span><span style=display:flex;background-color:#3d3f4a><span>bench_fft_g1 scale: &#39;7&#39; time:   [32.194 ms 32.471 ms 32.760 ms]
</span></span></code></pre></div><p>Yet the parallel version of the <code>fft_g1</code> algorithm performed much faster than it did for blst-from-scratch, even though both unparallelized versions for both teams performed evenly:</p><div class=highlight><pre tabindex=0 style=color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4;display:grid><code class=language-text data-lang=text><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;
</span></span><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;: Warming up for 3.0000 s
</span></span><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;: Collecting 100 samples in estimated 5.0701 s (300 iterations)
</span></span><span style=display:flex><span>Benchmarking bench_fft_g1 scale: &#39;7&#39;: Analyzing
</span></span><span style=display:flex;background-color:#3d3f4a><span>bench_fft_g1 scale: &#39;7&#39; time:   [16.854 ms 17.107 ms 17.439 ms]
</span></span><span style=display:flex;background-color:#3d3f4a><span>                        change: [-48.216% -47.318% -46.306%] (p = 0.00 &lt; 0.05)
</span></span><span style=display:flex><span>                        Performance has improved.
</span></span></code></pre></div><p>To cut a long story short, ckzg outperformed blst-from-scratch in all of the 7 benchmark groups.</p><h2 id=summary>Summary</h2><ul><li>OpenMP lets you quickly propotype what is feasible to parallelize with the help of CPU profiling tools such as Perf</li><li>Criterion is actually a really nice benchmarking tool to measure performance, especially when integrated into CI</li><li>ckzg has surpassed blst-from-scratch in becoming the fastest Rust library (yet) for kzg10 commitments</li></ul><div class=separator></div><br><div class=media><div class=media-left><img src=https://belijzajac.dev/author.jpg alt=selfie class="mr-3 mt-3 rounded-circle" style="border:2px solid #ddd;border-radius:50%"></div><div class=media-body><h4 class=media-heading>The author behind this post</h4>I'm a software developer with a diverse set of interests that include, but are not limited to, bare-metal assembly, Linux, and modern C++20. Since I was a kid, I've been using Linux (since that's what my toaster laptop could handle back in 2010), spreading FUD botnets, performing SQL injections on local WordPress websites, and more. I'm willing to collaborate on projects and can be reached at<div class=email>$ echo Z2l0aHViQGJlbGlqemFqYWMuZGV2Cg== | base64 -d</div></div></div></article><script type=application/javascript>countDownDate=new Date("2021-12-23");var x=setInterval(function(){var t=(new Date).getTime(),e=t-countDownDate.getTime(),n=Math.floor(e/(1e3*60*60*24)),s=Math.floor(e%(1e3*60*60*24)/(1e3*60*60)),o=Math.floor(e%(1e3*60*60)/(1e3*60)),i=Math.floor(e%(1e3*60)/1e3);document.getElementById("demo").innerHTML=n+"d "+s+"h "+o+"m "+i+"s "},1e3)</script><footer class=footer><span>Blog is running for <tag id=demo></tag><p>Copyright Â© 2022
<a href=https://github.com/belijzajac target=_blank rel=noopener>belijzajac</a></span></footer></div><script>window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)},ga.l=+new Date,ga("create","UA-217331412-1","auto"),ga("send","pageview")</script><script async src=https://www.google-analytics.com/analytics.js></script></body></html>